# TQC
Source codes for implementation of the Kitaev Honeycomb model!

## Codes Description

* show.py
    * a utility script to show .log file generated by netket, either gui or cui
    * see more usages with `./show.log --help`
* honeycomb.py
    * some utility functions which will be used in many other codes. It includes, get_graph_reza(), get_hamiltonian(), exact_diag(), gs_energy_babak(), rbm(), measure() and flip_spins()
    * you need to create machine yourself and pass it to rbm() to train it
    * This script is supposed to only be imported and used in other scripts. One of using demo of this script can be found in calc_gs.py
* exact_diag_for_reza.py
    * a script to show Reza that the exact diagonalize energy is different from Babak's formula
* test_flip_and_measure.py
    * test measure and flip technology by Ising model
* play_with_alpha.py
    * try how large of alpha is suitable, by a 5x5 lattice, 1kx10k trains and alpha 0.1,0.3,0.5,1,1.5,2,3,4,6,8,10.
* try_other_machines.py
    * try ffnn and compare it with rbm, from the perspective of time, effecient of parameters and, the most important, accuracy.
* vortex_energy.py
    * create vortex and measure its energy.
## Results

### Create Vortex and Measure its energy

There are three theoretically equivalent method to create vortices
* (a) modifing parameters of RBM
* (b) creating a auxiliary Hamiltonian and train again
* (c) transforming into auxiliary fermion and fixing u_{ij}s (Kitaev and Pachos)

I will first triple check using 3x3 lattice. The numeric result of gs energy of 3x3 lattice is -13.7166(2), the theoretical gs energy is -14.2915. The results is as the table below.

|spins be flipped|energy of a |energy of b |energy of c|
|:--------------:|:----------:|:----------:|:---------:|
|8xz             |-11.7335(5) |-12.1070(51)|-13.9146   |
|9xz             |-11.7474(5) |-11.8873(57)|-13.9146   |
|8y              |unrealizable|-12.7447(37)|-13.9146   |

__There is a wired thing in  method c, need to disscuss!__

Method a is more stable than method b. Both of method a and b is far from the correct result, method c. As the time they take, method c takes several ms to finish; method a and b take several minutes and method a is 3 or 4 times faster than method b.

Now turn to larger lattice size, say, 7x7. It is wired in computation of method a that all these computations take exactly 1.5 days. By exactly, I means the error is at most 4 mins. The results are as below.

|spins_be_flipped |/           |49xy       |49,51xy    |47,49,51xy |47,49,51,53xy|47xz48xy49yz|47xz48xy49yz 51xz52xy53yz|
|:---------------:|:----------:|:---------:|:---------:|:---------:|:-----------:|:----------:|:-----------------------:|
|vortices_distance|0           |1          |2          |3          |4            |2           |4                        |
|method_a_energy  |-71.7927(30)|-69.779(34)|-67.673(37)|-65.574(40)|-63.598(42)  |-70.053(39) |
|energy_diff      |/           |2.014      |4.120      |6.219      |8.195        |1.7397      |
|method_c_energy  |-77.1249    |-76.8307   |-76.8490   |-76.8308   |-76.8308     |-76.8490    |-76.8308                 |
|energy_diff      |/           |0.2942     |0.2759     |0.2941     |0.2941       |0.2759      |0.2941                   |

It is worth to evaluate energy of vertices by method c for more large systems.

|vortices_distance|0        |1        |2        |3        |4        |5        |6        |7        |8        |
|:---------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
|energy_for_7x7   |-77.1249 |-76.8307 |-76.8490 |-76.8308 |-76.8308 |-76.8490 |
|energy_diff      |/        |0.2942   |0.2759   |0.2941   |0.2941   |0.2759   |
|energy_for_25x25 |-984.1179|-983.8513|-983.8205|-983.8551|-983.8108|-983.8209|-983.8310|-983.8001|-983.8205|
|energy_diff      |/        |0.2666   |0.2974   |0.2628   |0.3071   |0.2970   |0.2869   |0.3178   |0.2974   |

### Ground State Energy

Up to now, we have too many methods to calculate gs energies, including
* a. Analytic formula using
    * i. anti-periodic boundry condition
    * ii. periodic boundry condition
* b. The method of Kitaev and Pachos
* c. Create Hamiltonian in Netket and direct diagnolize
* d. RBM (alpha=2, train by 1kx10k)
Here I will compare the gs energy of them with different lattice size

|lattice size|2x2    |2x3    |2x4     |3x3     |3x4     |4x4     |5x5     |6x6     |7x7     |8x8      |
|:----------:|:-----:|:-----:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:-------:|
|a.i         |-6.4721|-9.8003|-12.5851|-14.2915|-18.9869|-25.1282|-39.3892|-56.7529|-77.1249|-100.7799|
|a.ii        |-6     |-9.2915|-12.4721|-14.2915|-19.0918|-25.4164|-39.3892|-56.2668|-77.1249|-100.8009|
|b           |-6     |-9.2915|-12.4721|-14.2915|-19.0918|-25.4164|-39.3892|-56.2668|-77.1249|-100.8009|
|c           |-6.9282|-9.8003|-12.9443|-14.2915|-19.0918|
|d           |-6.7197|-9.5414|-12.573 |-13.7374|-18.0466|-24.0783|-37.305 |-52.920 |-71.793 |-93.755  |

From above table, it can be seen that method a.ii always agrees method b, which means we should use periodic boundry condition. It is still unknown why when lattice size is small method c and d's result are lower than method a and b.

#### Try to improve accuracy

Maybe first train RBM with large learning rate and then decrease it will lower gs energy. I keep train iters to 10k, but split it into two stages, first train with learning rate 0.3 then train with 0.01. But it comes out negetive result.

random init --1k--> -36.610 --9k--> -37.183</br>
random init --2k--> -36.631 --8k--> -37.101</br>
random init --3k--> -36.649 --7k--> -37.291</br>
random init --4k--> -36.697 --6k--> -37.285</br>
random init --5k--> -36.758 --5k--> -37.201</br>

The first training lr=0.3 is too large. Using 0.02 or 0.03 as first stage lr is suitable. So I first keep lr to 0.03 then lower to 0.01.

random init --1k--> -36.622 --9k--> -37.086</br>
random init --2k--> -36.624 --8k--> -37.163</br>
random init --3k--> -36.659 --7k--> -37.193</br>
random init --4k--> -36.688 --6k--> -37.204</br>
random init --5k--> -36.772 --5k--> -37.059</br>

### Play with alpha: how many hidden nodes are suitable?

Study how number of hidden nodes, denoted by num_nh, influences accuracy and training time. Fix the __lattice size to 5x5__, train batch number to 1kx10k, optimizer to Sgd(learning_rate=0.01,decay_factor=1), sampler to MetropolisLocal. The exact ground state energy of 5x5 lattice is -39.3892. In the table below, -27.7808(59) means $-27.7808\pm0.0059$.

|alpha|num_nh|num_para|energy|time(s)|notes|
|:---:|:----:|:------:|:----:|:--:|:---:|
|0.1|  5| 305|-27.7808(59)|171.6|
|0.3| 15| 815|-33.4312(37)|455.0|
|0.5| 25|1325|-36.6024(22)|711.9|
|1.0| 50|2600|-37.2464(13)|1549|
|1.5| 75|3875|-37.1235(15)|2764|
|2.0|100|5150|-37.3054(14)|5611|
|3.0|150|7700|-37.3037(14)|8415|
|4.0|200|10250|-37.1007(16)|11204|
|6.0|300|15350|-37.2989(14)|18167|
|8.0|400|20450|-37.1710(15)|26000|
|10.0|500|25550|-37.1946(15)|32056|

* __2 is the most suitable value for alpha__. If it is too low, the fitting performace is not good; too high overfit will happen.
* The training time is approximately proportional to the number of parameters. __For each additional parameter, the training time is increased by about 1 second.__
* __The result of RBM is still far away from the analytic result.__

### The performance of FFNN
Below is the result of FFNN with one layer. As the structure of ffnn is similar to that of a no-visible-bias RBM(reffered as novb RBM below), the performance of one-layer FFNN is compared to that of no-visible-bias RBM.

|alpha|num_nh|num_para|ffnn_energy|novb_rbm_energy|ffnn_time(s)|novb_rbm_time|notes|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|1.0| 50|2550|-37.2401(15)|-37.1532(15)|2783|2754|
|2.0|100|5100|-37.2958(14)|-36.8954(18)|4591|5476|
|3.0|150|7650|-37.3007(14)|-37.2191(15)|6889|8331|

* FFNN trains faster than RBM and performs better than RBM with no bias.
* But FFNN's result again far away from the analytic one.

Deep FFNNs with 2 and 3 layers are also tried. But some bugs seem happen, the energies stay at -25. It will be studied later.
