## Tomography

A typical tomography result is like

<img src="https://github.com/mrnp95/TQC/raw/master/sun_0306/2x2_a2.png" width="50%">

It is a tomography for 2x2 lattice. The parameters are: num_train_data=8x32x2000, alpha=2. The training datas are randomly generated by a groundstate.

There are many intricate parameters in training to change, such as learning rate, train_batch_num, sample_num, num_train_data. Only with suitable parameters can satisfactory output be obtained. The number of training data will affect the final precision tomography can reach. For example, the overlap can only reach 50% for 2x2 lattice when num_train_data=2000.

There is another interesting thing, which is, that only train_batch_num and sample_num both reach some thresholds can be the tomography success. Or its overlap will fluctuate around a very low level forever. I call this phenomenon "launch". For example, for 2x2 lattice, when train_batch_num=sample_num=1k or larger can it "launch". But when train_batch_num=sample_num=500, it cannot.


## Plaquette Operator

I realized flipping spins by modifying parameters of RBM before. Now we can verify this by flipping a spin and observe corresponding plaquette operators change or not.

For a rbm for 2x2 lattice with alpha=2 got by tomography, its four plaquette operators are computed,
    
    A: 0.94889473
    B: 0.95168581
    C: 0.95248031
    D: 0.95528330
    Energy: -6.0778
    Energy(exact): -6.4721

Then its 5th electron is flipped in yz direction, which will affect plaquette B and D. And its plaquette operators change to:
    
    A: 0.94889473
    B: -0.95168581
    C: 0.95248031
    D: -0.95528330
    Energy: -4.0105
    Energy(exact): -6.2925

Then its 7th electron is flipped (based on last step), which will affect plaquette A and D:

    A: -0.94889473
    B: -0.95168581
    C: 0.95248031
    D: 0.95528330
    Energy: -1.8040
    Energy(exact): -5.8064
    
Then its 3rd electron is flipped, which will affect plaquette A and B (i.e. back to initial state):

    A: 0.94889473
    B: 0.95168581
    C: 0.95248031
    D: 0.95528330
    Energy: 0.0606
    Energy(exact): -6.4721
    
In other words, the flipping I realizing before works perfectly. But the energy does not perform good: (i) the energy change when flipping a spin is too large in contrust to exact(about 2 to about 0.2); (ii) the energy does not get back to ground state energy after 3 flipps.

The machine parameters for above computation is in this folder, the "2x2_a2.best.ma".
    
## Add Plaquette Operators in training

Values of plaquette operators can be used as criteria and auxiliary targets in training. But, after three days of struggle and desperate, the result is not contenting.

If training only on energy and observe plaquette operators, their values will always be near zero. If train on maximizing the sum of plaquette operators, sum_plaq will be better, but the energy this time will fluctuate around zero. If train on energy minus sum_plaq, the situation will be a little better, but neither energy nor sum_plaq can be close to theoretical values. The typical results are, for 3x3 lattice, whose exact gs energy is -14.2915, if training only on energy, the energy can reach -13.7, but sum_plaq will stay at zero; if training on sum_plaq, it can get to 8.5 but energy will be zero; if train on both, their values will compromise, and finally be about -12 and 7.

My explanation for this is as follows. We know that near the ground state, there are many vortices states whose energies are only about 0.2 higher than ground state energy. Due to the stable of vortices states, they are all local minimum for energy. So when we try to find out its energy-minimal state, we also keep many states like this. Remember that vortices states have minus Ws; this explains why sum_plaq always be zero when we train on energy. Furthermore, this also explains why there is always a small distance from out rbm energy to exact energy.

To explain why energy becomes zero when training on sum_plaq, remember that the energy of honeycomb model is symmetry -- its ground states have sum_plaq =9. However, its highest energy states also have sum_plaq=9. So training on sum_plaq will pick these two types of states out, with nearly the same weights. That makes the energy be near zero.

I tried many combinations and methods as well as many configurations of parameters to utilize both energy and sum_plaq to make the result of rbm better but did not success so far.
    
