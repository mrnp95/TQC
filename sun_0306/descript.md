## Tomography

A typical tomography result is like

<img src="https://github.com/mrnp95/TQC/raw/master/sun_0306/2x2_a2.png" width="50%">

It is a tomography for 2x2 lattice. 
The parameters are: num_train_data=8x32x2000, alpha=2. 
The training datas are randomly generated by a groundstate. 
    
There are many intricate parameters in training to change, such as learning rate, train_batch_num, sample_num, num_train_data. 
Only with suitable parameters can satisfactory output be got.
The number of training datas will affect the final pricision tomography can reach.
For example, the overlap can only reach 50% for 2x2 lattice when num_train_data=2000.

There is another interesting thing, which is, that only train_batch_num and sample_num both reach some thresholds can the tomography success. Or its overlap will fluctuates around a very low level forever. I call this phenomenon "launch". For example, for 2x2 lattice, when train_batch_num=sample_num=1k or larger can it "launch". But when train_batch_num=sample_num=500, it cannot. 


## Plaquette Operator

I realized flipping spins by modifying parameters of RBM before. Now we can verify this by flipping a spin and observe corresponding plaquette operators change or not.

For a rbm for 2x2 lattice with alpha=2 got by tomography, its four plaquette operators are computed,
    
    A: 0.85984304
    B: 0.86507473
    C: 0.85363150
    D: 0.85877449

Then its 5th electron is flipped in yz direction, which will affect plaquette B and D. And its plaquette operators change to:
    
    A: 0.85984304
    B: -0.86507473
    C: 0.85363150
    D: -0.85877449

Then its 7th electron is flipped (based on last step), which will affect plaquette A and D:

    A: -0.85984304
    B: -0.86507473
    C: 0.85363150
    D: 0.85877449
    
Then its 3rd electron is flipped, which will affect plaquette A and B (i.e. back to initial state):

    A: 0.85984304
    B: 0.86507473
    C: 0.85363150
    D: 0.85877449
    
In other words, the flipping I realizing before works perfectly.
    
## Add Plaquette Operators in training

Values of plaquette operators can be used as criteria and auxiliary targets in training. But, after three days of struggle and desperate, the result is not contenting.

If training only on energy and observe plaquette operators, their values will always be near zero. If train on maximizing the sum of plaquette operators, sum_plaq will be better, but the energy this time will fluctuate around zero. If train on energy minus sum_plaq, the situation will be a little better, but neither energy nor sum_plaq can be close to theoretical values. The typical results are, for 3x3 lattice, whose exact gs energy is -14.2915, if training only on energy, the energy can reach -13.7, but sum_plaq will stay at zero; if training on sum_plaq, it can get to 8.5 but energy will be zero; if train on both, their values will compromise, and finally be about -12 and 7.

My explanation for this is as follows. We know that near the ground state, there are many vortices states whose energies are only about 0.2 higher than ground state energy. Due to the stable of vortices states, they are all local minimum for energy. So when we try to find out its energy-minimal state, we also keep many states like this. Remember that vortices states have minus Ws; this explains why sum_plaq always be zero when we train on energy. Furthermore, this also explains why there is always a small distance from out rbm energy to exact energy.

To explain why energy becomes zero when training on sum_plaq, remember that the energy of honeycomb model is symmetry -- its ground states have sum_plaq =9. However, its highest energy states also have sum_plaq=9. So training on sum_plaq will pick these two types of states out, with nearly the same weights. That makes the energy be near zero.

I tried many combinations and methods as well as many configurations of parameters to utilize both energy and sum_plaq to make the result of rbm better but did not success so far.
    
